{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sribhashyamsashanksai/machine-learning/blob/main/Deep_Learning_based_Recommender_Systems.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'movielens-20m-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F339%2F77759%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240801%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240801T161723Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D482213266c352b30b6399d5883136bc826195f1db8a01714e126f3d115b00f06bda7819c615b3fbc792d87f55930b89b6fe62c2b8572044cfdbe02b1df8023d4b2dca8da3d798b99527d9b5195c736bfce44a81adba8e38e18da05cf2a13ef5f32805bb969e6ab7ace576ce4de7bc6abbdd3f1f22ca867a06a39dd81b23a2cf99d5d49eecf9de6e3b01df8799762f6985c943b41996a3adb90a948e9b36bc7c40fbfde91ca1bf049255caa0e38b08c9ca051e3b0cec6d4438fd18bcc57835b0b16f13a4466a28f79b446fd4e24bbcdf6b9e955920554adee29f3ce1dfec08681c038613d75b5d4ba961c3c710898ea6bb6a46f3c06bf70b7d299d7c61adb4cc6'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "zJGlwth9u122"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "jpkzDIf7u127"
      },
      "cell_type": "markdown",
      "source": [
        "# Deep Learning based Recommender System\n",
        "\n",
        "Traditionally, recommender systems are based on methods such as clustering, nearest neighbor and matrix factorization. However, in recent years, deep learning has yielded tremendous success across multiple domains, from image recognition to natural language processing. Recommender systems have also benefited from deep learning's success. In fact, today's state-of-the-art recommender systems such as those at [YouTube](https://research.google/pubs/pub45530/) and [Amazon](https://www.amazon.science/the-history-of-amazons-recommendation-algorithm) are powered by complex deep learning systems, and less so on traditional methods."
      ]
    },
    {
      "metadata": {
        "id": "uat-NZq-u12-"
      },
      "cell_type": "markdown",
      "source": [
        "# Why this notebook?\n",
        "\n",
        "While reading through the many useful notebooks here that covers the basics of recommender systems using traditional methods such as matrix factorization, I noticed that there is a lack of tutorials that cover deep learning based recommender systems. In this notebook, we'll go through the following:\n",
        "\n",
        "* How to create your own deep learning based recommender system using PyTorch Lightning\n",
        "* The difference between implicit and explicit feedback for recommender systems\n",
        "* How to train-test split a dataset for training recommender systems without introducing biases and data leakages\n",
        "* Metrics for evaluating recommender systems (hint: accuracy or RMSE is not appropriate!)"
      ]
    },
    {
      "metadata": {
        "id": "mxZ4Yykfu12_"
      },
      "cell_type": "markdown",
      "source": [
        "# Building Recommender Systems using Implicit Feedback\n",
        "\n",
        "Before we build our model, it is important to understand the distinction between *implicit* and *explicit* feedback in the context of recommender systems, and why modern recommender systems are built on implicit feedback.\n",
        "\n",
        "### Explicit Feedback\n",
        "\n",
        "In the context of recommender systems, explicit feedback are **direct** and **quantitative** data collected from users. For example, Amazon allows users to rate purchased items on a scale of 1-10. These ratings are provided directly from users, and the scale allows Amazon to quantify user preference. Another example of explicit feedback includes the thumbs up button on YouTube, which captures users' explicit preference (i.e. like or dislike) of a particular video.\n",
        "\n",
        "However, the problem with explicit feedback is that they are rare. If you think about it, when was the last time you clicked the like button on a YouTube video, or rated your online purchases? Chances are, the amount of videos you watch on YouTube is far greater than the amount of videos that you have explicitly rated.\n",
        "\n",
        "### Implicit Feedback\n",
        "\n",
        "On the other hand, implicit feedback are collected indirectly from user **interactions**, and they act as a proxy for user preference. For example. videos that you watch on YouTube are used as implicit feedback to tailor recommendations for you, even if you don't rate the videos explicitly. Another example of implicit feedback includes the items that you have browsed on Amazon, which are used to suggest other similar items for you.\n",
        "\n",
        "The advantage of implicit feedback is that it is abundant. Recommender systems built using implicit feedback also allows us to tailor recommendations in real time, with every click and interaction. Today, online recommender systems are built using implicit feedback, which allows the system to tune its recommendation in real-time, with every user interaction.\n",
        "\n",
        "However, implicit feedback has its shortcomings as well. Unlike explicit feedback, every interaction is assumed to be positive and we are unable to capture negative preference from users. How then do we capture negative feedback? One technique that can be applied is negative sampling, which we will go through in a later section."
      ]
    },
    {
      "metadata": {
        "id": "r9US3LLju13A"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "Before we start building and training our model, let's do some preprocessing to get the data in the required format."
      ]
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "id": "5hoh6UVuu13A"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "np.random.seed(123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WYdESBoNu13B"
      },
      "cell_type": "markdown",
      "source": [
        "First, we import the ratings dataset."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "-HmnDThyu13C"
      },
      "cell_type": "code",
      "source": [
        "ratings = pd.read_csv('/kaggle/input/movielens-20m-dataset/rating.csv',\n",
        "                      parse_dates=['timestamp'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5RgAh9LFu13C"
      },
      "cell_type": "markdown",
      "source": [
        "In order to keep memory usage manageable within Kaggle's kernel, we will only use data from 30% of the users in this dataset. Let's randomly select 30% of the users and only use data from the selected users."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "UJkaeBmzu13D"
      },
      "cell_type": "code",
      "source": [
        "rand_userIds = np.random.choice(ratings['userId'].unique(),\n",
        "                                size=int(len(ratings['userId'].unique())*0.3),\n",
        "                                replace=False)\n",
        "\n",
        "ratings = ratings.loc[ratings['userId'].isin(rand_userIds)]\n",
        "\n",
        "print('There are {} rows of data from {} users'.format(len(ratings), len(rand_userIds)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "1xcUNF1Wu13E"
      },
      "cell_type": "code",
      "source": [
        "ratings.sample(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xiGr8ZzQu13E"
      },
      "cell_type": "markdown",
      "source": [
        "After filtering the dataset, there are now 6,027,314  rows of data from 41,547 users. Each row in the dataframe corresponds to a movie review made by a single user."
      ]
    },
    {
      "metadata": {
        "id": "KHK3Tznxu13F"
      },
      "cell_type": "markdown",
      "source": [
        "### Train-test split\n",
        "\n",
        "Along with the rating, there is also a `timestamp` column that shows the date and time the review was submitted. Using the `timestamp` column, we will implement our train-test split strategy using the leave-one-out methodology. For each user, the most recent review is used as the test set (i.e. leave one out), while the rest will be used as training data .\n",
        "\n",
        "To illustrate this, the movies reviewed by user 39,849 is shown below. The last movie reviewed by the user is the 2014 hit movie Guardians of The Galaxy. We'll use this movie as the testing data for this user, and use the rest of the reviewed movies as training data.\n",
        "\n",
        "![](https://i.imgur.com/oNJnLqU.png)\n",
        "> **Movie posters from themoviedb.org (free to use)**\n",
        ">\n",
        "\n",
        "\n",
        "\n",
        "This train-test split strategy is often used when training and evaluating recommender systems. Doing a random split would not be fair, as we could potentially be using a user's recent reviews for training and earlier reviews for testing. This introduces data leakage with a look-ahead bias, and the performance of the trained model would not be generalizable to real-world performance.\n",
        "\n",
        "The code below will split our ratings dataset into a train and test set using the leave-one-out methodology."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "q58WTVLCu13F"
      },
      "cell_type": "code",
      "source": [
        "ratings['rank_latest'] = ratings.groupby(['userId'])['timestamp'] \\\n",
        "                                .rank(method='first', ascending=False)\n",
        "\n",
        "train_ratings = ratings[ratings['rank_latest'] != 1]\n",
        "test_ratings = ratings[ratings['rank_latest'] == 1]\n",
        "\n",
        "# drop columns that we no longer need\n",
        "train_ratings = train_ratings[['userId', 'movieId', 'rating']]\n",
        "test_ratings = test_ratings[['userId', 'movieId', 'rating']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c5-KA8mou13F"
      },
      "cell_type": "markdown",
      "source": [
        "### Converting the dataset into an implicit feedback dataset\n",
        "\n",
        "As discussed earlier, we will train a recommender system using implicit feedback. However, the MovieLens dataset that we're using is based on explicit feedback. To convert this dataset into an implicit feedback dataset, we'll simply binarize the ratings such that they are are '1' (i.e. positive class). The value of '1' represents that the user has interacted with the item.\n",
        "\n",
        "It is important to note that using implicit feedback reframes the problem that our recommender is trying to solve. Instead of trying to predict movie ratings (when using explicit feedback), we are trying to predict whether the user will interact (i.e. click/buy/watch) with each movie, with the aim of presenting to users the movies with the highest interaction likelihood.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "40v3S9j8u13G"
      },
      "cell_type": "code",
      "source": [
        "train_ratings.loc[:, 'rating'] = 1\n",
        "\n",
        "train_ratings.sample(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KGYVJ2Qiu13G"
      },
      "cell_type": "markdown",
      "source": [
        "We do have a problem now though. After binarizing our dataset, we see that every sample in the dataset now belongs to the positive class. However we also require negative samples to train our models, to indicate movies that the user has not interacted with. We assume that such movies are those that the user are not interested in - even though this is a sweeping assumption that may not be true, it usually works out rather well in practice.\n",
        "\n",
        "The code below generates 4 negative samples for each row of data. In other words, the ratio of negative to positive samples is 4:1. This ratio is chosen arbitrarily but I found that it works rather well (feel free to find the best ratio yourself!)"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "hNe-fLxPu13H"
      },
      "cell_type": "code",
      "source": [
        "# Get a list of all movie IDs\n",
        "all_movieIds = ratings['movieId'].unique()\n",
        "\n",
        "# Placeholders that will hold the training data\n",
        "users, items, labels = [], [], []\n",
        "\n",
        "# This is the set of items that each user has interaction with\n",
        "user_item_set = set(zip(train_ratings['userId'], train_ratings['movieId']))\n",
        "\n",
        "# 4:1 ratio of negative to positive samples\n",
        "num_negatives = 4\n",
        "\n",
        "for (u, i) in tqdm(user_item_set):\n",
        "    users.append(u)\n",
        "    items.append(i)\n",
        "    labels.append(1) # items that the user has interacted with are positive\n",
        "    for _ in range(num_negatives):\n",
        "        # randomly select an item\n",
        "        negative_item = np.random.choice(all_movieIds)\n",
        "        # check that the user has not interacted with this item\n",
        "        while (u, negative_item) in user_item_set:\n",
        "            negative_item = np.random.choice(all_movieIds)\n",
        "        users.append(u)\n",
        "        items.append(negative_item)\n",
        "        labels.append(0) # items not interacted with are negative"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VdOn1qxQu13H"
      },
      "cell_type": "markdown",
      "source": [
        "Great! We now have the data in the format required by our model. Before we move on, let's define a PyTorch Dataset to facilitate training. The class below simply encapsulates the code we have written above into a PyTorch Dataset class."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "jaSGmyUou13I"
      },
      "cell_type": "code",
      "source": [
        "class MovieLensTrainDataset(Dataset):\n",
        "    \"\"\"MovieLens PyTorch Dataset for Training\n",
        "\n",
        "    Args:\n",
        "        ratings (pd.DataFrame): Dataframe containing the movie ratings\n",
        "        all_movieIds (list): List containing all movieIds\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ratings, all_movieIds):\n",
        "        self.users, self.items, self.labels = self.get_dataset(ratings, all_movieIds)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.users)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.users[idx], self.items[idx], self.labels[idx]\n",
        "\n",
        "    def get_dataset(self, ratings, all_movieIds):\n",
        "        users, items, labels = [], [], []\n",
        "        user_item_set = set(zip(ratings['userId'], ratings['movieId']))\n",
        "\n",
        "        num_negatives = 4\n",
        "        for u, i in user_item_set:\n",
        "            users.append(u)\n",
        "            items.append(i)\n",
        "            labels.append(1)\n",
        "            for _ in range(num_negatives):\n",
        "                negative_item = np.random.choice(all_movieIds)\n",
        "                while (u, negative_item) in user_item_set:\n",
        "                    negative_item = np.random.choice(all_movieIds)\n",
        "                users.append(u)\n",
        "                items.append(negative_item)\n",
        "                labels.append(0)\n",
        "\n",
        "        return torch.tensor(users), torch.tensor(items), torch.tensor(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Ao5RSHDu13I"
      },
      "cell_type": "markdown",
      "source": [
        "# Our model - Neural Collaborative Filtering (NCF)\n",
        "\n",
        "While there are many deep learning based architecture for recommendation systems, I find that the framework proposed by [He et al.](https://arxiv.org/abs/1708.05031) is the most straightforward and it is simple enough to be implemented in a tutorial such as this.\n",
        "\n",
        "### User Embeddings\n",
        "\n",
        "Before we dive into the architecture of the model, let's familiarize ourselves with the concept of embeddings. An embedding is a low-dimensional space that captures the relationship of vectors from a higher dimensional space. To better understand this concept, let's take a closer look at user embeddings.\n",
        "\n",
        "Imagine that we want to represent our users according to their preference for two genres of movies - action and romance movies. Let the first dimension be how much the user likes action movies, and the second dimension be how much the user likes romance movies.\n",
        "\n",
        "![](https://i.imgur.com/XENzqXq.png)\n",
        "\n",
        "Now, assume that Bob is our first user. Bob likes action movies but isn't a fan of romance movies. To represent Bob as a two dimensional vector, we place him in the graph according to his preference.\n",
        "\n",
        "![](https://i.imgur.com/rSStTCj.png)\n",
        "\n",
        "Our next user is Joe. Joe is a huge fan of both action and romance movies. We represent Joe using a two dimensional vector just like Bob.\n",
        "\n",
        "![](https://i.imgur.com/gmmkrEU.png)\n",
        "\n",
        "This two dimensional space is known as an embedding. Essentially, the embedding reduces our users such that they can be represented in a meaningful manner in a lower dimensional space. In this embedding, users with similar movie preferences are placed near to each other, and vice versa.\n",
        "\n",
        "![](https://i.imgur.com/9s9Z7JT.png)\n",
        "\n",
        "Of course, we are not restricted to using just 2 dimensions to represent our users. We can use an arbitrary number of dimensions to represent our users. A larger number of dimensions would allow us to capture the traits of each user more accurately, at the cost of model complexity. In our code, we'll use 8 dimensions (which we will see later).\n",
        "\n",
        "### Learned Embeddings\n",
        "\n",
        "Similarly, we will use a separate item embedding layer to represent the traits of the items (i.e. movies) in a lower dimensional space.\n",
        "\n",
        "You might be wondering, how can we learn the weights of the embedding layer, such that it provides an accurate representation of users and items? In our previous example, we used Bob and Joe's preference for action and romance movies to manually create our embedding. Is there a way to learn such preferences automatically?\n",
        "\n",
        "The answer is **Collaborative Filtering** - by using the ratings dataset, we can identify similar users and movies, creating user and item embeddings learned from existing ratings.\n",
        "\n",
        "### Model Architecture\n",
        "\n",
        "Now that we have a better understanding of embeddings, we are ready to define the model architecture. As you'll see, the user and item embeddings are key to the model.\n",
        "\n",
        "<!-- ![NCF](https://i.imgur.com/EZh1HHf.png)\n",
        " -->\n",
        "\n",
        "Let's walk through the model architecture using the following training sample:\n",
        "\n",
        "| userId | movieID | interacted |\n",
        "|-|-|-|\n",
        "| 3 | 1 | 1 |\n",
        "\n",
        "\n",
        "![](https://i.imgur.com/cNWbIce.png)\n",
        "\n",
        "\n",
        "The inputs to the model are the one-hot encoded user and item vector for `userId = 3` and `movieId = 1`. Because this is a positive sample (movie actually rated by the user), the true label (`interacted`) is 1.\n",
        "\n",
        "The user input vector and item input vector are fed to the user embedding and item embedding respectively, which results in a smaller, denser user and item vectors.\n",
        "\n",
        "The embedded user and item vectors are concatenated before passing through a series of fully connected layers, which maps the concatenated embeddings into a prediction vector as output. Finally, we apply a `Sigmoid` function to obtain the most probable class. In the example above, the most probable class is 1 (positive class), since 0.8 > 0.2.\n",
        "\n",
        "\n",
        "Now, let's define this NCF model using PyTorch Lightning!"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "EISp8mQnu13J"
      },
      "cell_type": "code",
      "source": [
        "class NCF(pl.LightningModule):\n",
        "    \"\"\" Neural Collaborative Filtering (NCF)\n",
        "\n",
        "        Args:\n",
        "            num_users (int): Number of unique users\n",
        "            num_items (int): Number of unique items\n",
        "            ratings (pd.DataFrame): Dataframe containing the movie ratings for training\n",
        "            all_movieIds (list): List containing all movieIds (train + test)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_users, num_items, ratings, all_movieIds):\n",
        "        super().__init__()\n",
        "        self.user_embedding = nn.Embedding(num_embeddings=num_users, embedding_dim=8)\n",
        "        self.item_embedding = nn.Embedding(num_embeddings=num_items, embedding_dim=8)\n",
        "        self.fc1 = nn.Linear(in_features=16, out_features=64)\n",
        "        self.fc2 = nn.Linear(in_features=64, out_features=32)\n",
        "        self.output = nn.Linear(in_features=32, out_features=1)\n",
        "        self.ratings = ratings\n",
        "        self.all_movieIds = all_movieIds\n",
        "\n",
        "    def forward(self, user_input, item_input):\n",
        "\n",
        "        # Pass through embedding layers\n",
        "        user_embedded = self.user_embedding(user_input)\n",
        "        item_embedded = self.item_embedding(item_input)\n",
        "\n",
        "        # Concat the two embedding layers\n",
        "        vector = torch.cat([user_embedded, item_embedded], dim=-1)\n",
        "\n",
        "        # Pass through dense layer\n",
        "        vector = nn.ReLU()(self.fc1(vector))\n",
        "        vector = nn.ReLU()(self.fc2(vector))\n",
        "\n",
        "        # Output layer\n",
        "        pred = nn.Sigmoid()(self.output(vector))\n",
        "\n",
        "        return pred\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        user_input, item_input, labels = batch\n",
        "        predicted_labels = self(user_input, item_input)\n",
        "        loss = nn.BCELoss()(predicted_labels, labels.view(-1, 1).float())\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters())\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(MovieLensTrainDataset(self.ratings, self.all_movieIds),\n",
        "                          batch_size=512, num_workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0WRbnlV1u13K"
      },
      "cell_type": "markdown",
      "source": [
        "We instantiate the NCF model using the class that we have defined above."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Y5t9VylVu13K"
      },
      "cell_type": "code",
      "source": [
        "num_users = ratings['userId'].max()+1\n",
        "num_items = ratings['movieId'].max()+1\n",
        "\n",
        "all_movieIds = ratings['movieId'].unique()\n",
        "\n",
        "model = NCF(num_users, num_items, train_ratings, all_movieIds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nxL6DQo6u13L"
      },
      "cell_type": "markdown",
      "source": [
        "Let's train our NCF model for 5 epochs using the GPU. Notice that we are using the argument `reload_dataloaders_every_epoch=True`. This creates a new randomly chosen set of negative samples for each epoch, which ensures that our model is not biased by the selection of negative samples.\n",
        "\n",
        "Note: One advantage of [PyTorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning) over vanilla PyTorch is that you don't need to write your own boiler plate training code. Notice how the [Trainer](https://pytorch-lightning.readthedocs.io/en/latest/trainer.html) class allows us to train our model with just a few lines of code."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "M5ZUyIWPu13L"
      },
      "cell_type": "code",
      "source": [
        "trainer = pl.Trainer(max_epochs=5, gpus=1, reload_dataloaders_every_epoch=True,\n",
        "                     progress_bar_refresh_rate=50, logger=False, checkpoint_callback=False)\n",
        "\n",
        "trainer.fit(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QSWcM1g9u13L"
      },
      "cell_type": "markdown",
      "source": [
        "# Evaluating our Recommender System\n",
        "\n",
        "Now that our model is trained, we are ready to evaluate it using the test data. In traditional Machine Learning projects, we evaluate our models using metrics such as Accuracy (for classification problems) and RMSE (for regression problems). However, such metrics are too simplistic for evaluating recommender systems.\n",
        "\n",
        "To design a good metric for evaluating recommender systems, we need to first understand how modern recommender systems are used.\n",
        "\n",
        "Looking at Netflix, we see a list of recommendations like the one below:\n",
        "\n",
        "![](https://i.imgur.com/5QRWcYy.jpg)\n",
        "\n",
        "Similarly, Amazon uses a list of recommendations:\n",
        "\n",
        "![](https://i.imgur.com/XZZ2Ni8.png)\n",
        "\n",
        "The key here is that we don't need the user to interact on *every* single item in the list of recommendations. Instead, we just need the user to interact with at least one item on the list - as long as the user does that, the recommendations have worked.\n",
        "\n",
        "To simulate this, let's run the following evaluation protocol to generate a list of 10 recommended items for each user.\n",
        "\n",
        "* For each user, randomly select 99 items that the user **has not interacted with**\n",
        "* Combine these 99 items with the test item (the actual item that the user interacted with). We now have 100 items.\n",
        "* Run the model on these 100 items, and rank them according to their predicted probabilities\n",
        "* Select the top 10 items from the list of 100 items. If the test item is present within the top 10 items, then we say that this is a hit.\n",
        "* Repeat the process for all users. The Hit Ratio is then the average hits.\n",
        "\n",
        "This evaluation protocol is known as **Hit Ratio @ 10**, and it is commonly used to evaluate recommender systems."
      ]
    },
    {
      "metadata": {
        "id": "miJhZAIcu13L"
      },
      "cell_type": "markdown",
      "source": [
        "### Hit Ratio @ 10\n",
        "\n",
        "Now, let's evaluate our model using the described protocol."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "JX4PKyPTu13M"
      },
      "cell_type": "code",
      "source": [
        "# User-item pairs for testing\n",
        "test_user_item_set = set(zip(test_ratings['userId'], test_ratings['movieId']))\n",
        "\n",
        "# Dict of all items that are interacted with by each user\n",
        "user_interacted_items = ratings.groupby('userId')['movieId'].apply(list).to_dict()\n",
        "\n",
        "hits = []\n",
        "for (u,i) in tqdm(test_user_item_set):\n",
        "    interacted_items = user_interacted_items[u]\n",
        "    not_interacted_items = set(all_movieIds) - set(interacted_items)\n",
        "    selected_not_interacted = list(np.random.choice(list(not_interacted_items), 99))\n",
        "    test_items = selected_not_interacted + [i]\n",
        "\n",
        "    predicted_labels = np.squeeze(model(torch.tensor([u]*100),\n",
        "                                        torch.tensor(test_items)).detach().numpy())\n",
        "\n",
        "    top10_items = [test_items[i] for i in np.argsort(predicted_labels)[::-1][0:10].tolist()]\n",
        "\n",
        "    if i in top10_items:\n",
        "        hits.append(1)\n",
        "    else:\n",
        "        hits.append(0)\n",
        "\n",
        "print(\"The Hit Ratio @ 10 is {:.2f}\".format(np.average(hits)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rlJjHcHJu13M"
      },
      "cell_type": "markdown",
      "source": [
        "We got a pretty good Hit Ratio @ 10 score! To put this into context, what this means is that 86% of the users were recommended the actual item (among a list of 10 items) that they eventually interacted with. Not bad!"
      ]
    },
    {
      "metadata": {
        "id": "Yi0Ui-HDu13M"
      },
      "cell_type": "markdown",
      "source": [
        "# What's Next?\n",
        "\n",
        "I hope that this has been an useful introduction to creating a deep learning based recommender systems. To learn more, I recommend the following resources:\n",
        "* [Wide & Deep Learning - Model introduced by Google for Recommender Systems](https://ai.googleblog.com/2016/06/wide-deep-learning-better-together-with.html)\n",
        "* [Recommenders library by Microsoft - Best practices for Recommender Systems](https://github.com/microsoft/recommenders)\n",
        "* [Deep Learning based Recommender Systems - Useful survey paper](https://arxiv.org/pdf/1707.07435.pdf)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Deep Learning based Recommender Systems",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}